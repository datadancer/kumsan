diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index 2418804..f748110 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -298,7 +298,9 @@ __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 	nr &= __SYSCALL_MASK;
 	if (likely(nr < NR_syscalls)) {
 		nr = array_index_nospec(nr, NR_syscalls);
+        kasan_poison_task_stack(current);
 		regs->ax = sys_call_table[nr](regs);
+        kasan_unpoison_task_stack(current);
 	}
 
 	syscall_return_slowpath(regs);
diff --git a/fs/read_write.c b/fs/read_write.c
index c543d96..af068ed 100644
--- a/fs/read_write.c
+++ b/fs/read_write.c
@@ -991,6 +991,8 @@ ssize_t vfs_readv(struct file *file, const struct iovec __user *vec,
 	struct iovec *iov = iovstack;
 	struct iov_iter iter;
 	ssize_t ret;
+    //Lu Shuaibing
+    memset(&iter, 0, sizeof(struct iov_iter));
 
 	ret = import_iovec(READ, vec, vlen, ARRAY_SIZE(iovstack), &iov, &iter);
 	if (ret >= 0) {
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index b40ea10..cddd39f 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -37,6 +37,7 @@ extern void kasan_disable_current(void);
 
 void kasan_unpoison_shadow(const void *address, size_t size);
 
+void kasan_poison_task_stack(struct task_struct *task);
 void kasan_unpoison_task_stack(struct task_struct *task);
 void kasan_unpoison_stack_above_sp_to(const void *watermark);
 
@@ -87,6 +88,7 @@ void kasan_restore_multi_shot(bool enabled);
 
 static inline void kasan_unpoison_shadow(const void *address, size_t size) {}
 
+static inline void kasan_poison_task_stack(struct task_struct *task) {}
 static inline void kasan_unpoison_task_stack(struct task_struct *task) {}
 static inline void kasan_unpoison_stack_above_sp_to(const void *watermark) {}
 
diff --git a/lib/test_kasan.c b/lib/test_kasan.c
index 7de2702..bc9e7ac 100644
--- a/lib/test_kasan.c
+++ b/lib/test_kasan.c
@@ -439,7 +439,7 @@ static noinline void __init copy_user_test(void)
 {
 	char *kmem;
 	char __user *usermem;
-	size_t size = 10;
+	size_t size = 60;
 	int unused;
 
 	kmem = kmalloc(size, GFP_KERNEL);
@@ -455,12 +455,10 @@ static noinline void __init copy_user_test(void)
 		return;
 	}
 
-	pr_info("out-of-bounds in copy_from_user()\n");
-	unused = copy_from_user(kmem, usermem, size + 1);
-
-	pr_info("out-of-bounds in copy_to_user()\n");
-	unused = copy_to_user(usermem, kmem, size + 1);
+	pr_info("uninitilize memory in copy_to_user()\n");
+	unused = copy_to_user(usermem, kmem, size);
 
+    return;
 	pr_info("out-of-bounds in __copy_from_user()\n");
 	unused = __copy_from_user(kmem, usermem, size + 1);
 
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 242fdc0..eb4378d 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -119,11 +119,26 @@ void *memmove(void *dest, const void *src, size_t len)
 }
 
 #undef memcpy
+void poison_propagate(void *dest, void *src, size_t len){     
+    u8 *shadow; size_t i = 0;     
+    while(i<len){                
+        shadow = (u8 *)kasan_mem_to_shadow(dest);     
+        if(memory_is_poisoned_by_bits((unsigned long)src, 1)){     
+            *shadow = (*shadow) | (0x80 >> (((unsigned long)dest) & KASAN_SHADOW_MASK));     
+        } else {                                                
+            *shadow = (*shadow) & ~(0x80 >> (((unsigned long)dest) & KASAN_SHADOW_MASK));     
+        }                                                               
+        i++; dest++; src++;     
+    }                          
+}   
+
 void *memcpy(void *dest, const void *src, size_t len)
 {
-	check_memory_region((unsigned long)src, len, false, _RET_IP_);
-	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
-
+    if(memory_is_poisoned_by_bits((unsigned long)src, len)){     
+        poison_propagate(dest, src, len);                     
+    } else {                                                   
+        check_memory_region((unsigned long)dest, len, true, _RET_IP_);
+    }
 	return __memcpy(dest, src, len);
 }
 
@@ -171,12 +186,28 @@ void kasan_unpoison_shadow(const void *address, size_t size)
 	}
 }
 
+static void __kasan_poison_stack(struct task_struct *task, const void *sp)
+{
+    void *base = task_stack_page(task) + sizeof(struct task_struct);
+    size_t size = sp - base - 64;
+
+	kasan_poison_shadow(base, size, 0xff);
+    __memset(base, KASAN_TAG_TAINT, size);
+}
+
+/* Unpoison the entire stack for a task. */
+void kasan_poison_task_stack(struct task_struct *task)
+{
+	__kasan_poison_stack(task, current_stack_pointer);
+}
+
 static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
 {
-	void *base = task_stack_page(task);
-	size_t size = sp - base;
+	void *base = task_stack_page(task) + sizeof(struct task_struct);
+	size_t size = sp - base - 64;
 
 	kasan_unpoison_shadow(base, size);
+    //__memset(base, 0, size);
 }
 
 /* Unpoison the entire stack for a task. */
@@ -485,6 +516,11 @@ static void *__kasan_kmalloc(struct kmem_cache *cache, const void *object,
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_KMALLOC_REDZONE);
 
+    if (!(flags & __GFP_ZERO) && cache->ctor == NULL){
+        __memset(object, KASAN_TAG_TAINT, size);
+	    kasan_poison_shadow(object, size, 0xff);
+    }
+
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
 
@@ -542,7 +578,7 @@ void * __must_check kasan_krealloc(const void *object, size_t size, gfp_t flags)
 		return kasan_kmalloc_large(object, size, flags);
 	else
 		return __kasan_kmalloc(page->slab_cache, object, size,
-						flags, true);
+						flags | __GFP_ZERO , true);
 }
 
 void kasan_poison_kfree(void *ptr, unsigned long ip)
diff --git a/mm/kasan/generic.c b/mm/kasan/generic.c
index 504c793..27c3ac6 100644
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@ -165,6 +165,29 @@ static __always_inline bool memory_is_poisoned(unsigned long addr, size_t size)
 
 	return memory_is_poisoned_n(addr, size);
 }
+//Fill the correspinding shadow bits as ZERO.      
+void kasan_unpoison_shadow_by_bits(const void *address, size_t size){     
+    size_t i; u8 *shadow;        
+    //Low efficiency, unpoison every bit one by one.      
+    for(i=0;i<size;i++){     
+        //Get the corresponding shadow address.      
+        shadow = (u8 *)kasan_mem_to_shadow(address + i);     
+        *shadow = (*shadow) & ~(0x80 >> (((unsigned long)address + i) & KASAN_SHADOW_MASK));
+    }
+}
+
+bool memory_is_poisoned_by_bits(unsigned long addr, size_t size){
+    size_t i; u8 *shadow; u8 shadow_byte;
+
+    //Low efficiency, unpoison every bit one by one.
+    for(i=0;i<size;i++){
+        //Get the corresponding shadow address.
+        shadow = (u8 *)kasan_mem_to_shadow((void *)addr + i);
+        shadow_byte = (*shadow) & (0x80 >> ((addr + i) & KASAN_SHADOW_MASK));
+        if(shadow_byte) return true;
+    }
+    return false;
+}
 
 static __always_inline void check_memory_region_inline(unsigned long addr,
 						size_t size, bool write,
@@ -179,10 +202,11 @@ static __always_inline void check_memory_region_inline(unsigned long addr,
 		return;
 	}
 
-	if (likely(!memory_is_poisoned(addr, size)))
+	if (likely(!memory_is_poisoned_by_bits(addr, size)))
 		return;
 
-	kasan_report(addr, size, write, ret_ip);
+	if (write) kasan_unpoison_shadow_by_bits(addr, size);
+    else kasan_report(addr, size, write, ret_ip);
 }
 
 void check_memory_region(unsigned long addr, size_t size, bool write,
diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index 36c6459..f165d49 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -36,11 +36,23 @@
 
 void *find_first_bad_addr(void *addr, size_t size)
 {
-	void *p = addr;
-
-	while (p < addr + size && !(*(u8 *)kasan_mem_to_shadow(p)))
-		p += KASAN_SHADOW_SCALE_SIZE;
-	return p;
+   size_t i, count; u8 shadow_val; void *last_aa = NULL;
+   for(i=0, count=0;i<size;i++){    
+       if (((unsigned long)addr & KASAN_SHADOW_MASK) == 0) count = 0;
+       if (*(u8 *)addr == KASAN_TAG_TAINT){ 
+           shadow_val = *(u8 *)kasan_mem_to_shadow(addr);    
+           if (shadow_val & (0x80 >> ((unsigned long)addr & KASAN_SHADOW_MASK))){    
+               /* Skip the one byte noise */     
+               if (shadow_val != 0xFF || size == 1) return addr;    
+               /* If shadow is FF, it means most of the byte should be aa */
+               if ((last_aa+1) == addr && count >= 4) return addr;
+           }
+           last_aa = addr;
+           count++;
+       }
+       addr++;    
+   }
+   return NULL; 
 }
 
 static const char *get_shadow_bug_type(struct kasan_access_info *info)
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 3ce956e..5a5ce58 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -8,6 +8,8 @@
 #define KASAN_SHADOW_SCALE_SIZE (1UL << KASAN_SHADOW_SCALE_SHIFT)
 #define KASAN_SHADOW_MASK       (KASAN_SHADOW_SCALE_SIZE - 1)
 
+#define KASAN_TAG_TAINT	0xbb /* native kernel pointers tag */
+
 #define KASAN_TAG_KERNEL	0xFF /* native kernel pointers tag */
 #define KASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
 #define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
@@ -121,6 +123,10 @@ static inline bool addr_has_shadow(const void *addr)
 	return (addr >= kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
 }
 
+bool memory_is_poisoned_by_bits(unsigned long addr, size_t size);
+
+void kasan_unpoison_shadow_by_bits(const void *address, size_t size);
+
 void kasan_poison_shadow(const void *address, size_t size, u8 value);
 
 void check_memory_region(unsigned long addr, size_t size, bool write,
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 03a4435..88685d0 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -220,11 +220,12 @@ static int shadow_pointer_offset(const void *row, const void *shadow)
 		(shadow - row) / SHADOW_BYTES_PER_BLOCK + 1;
 }
 
-static void print_shadow_for_address(const void *addr)
+static void print_shadow_for_address(const void *addr, bool is_content)
 {
 	int i;
-	const void *shadow = kasan_mem_to_shadow(addr);
 	const void *shadow_row;
+	const void *shadow = kasan_mem_to_shadow(addr);
+    if (is_content) shadow = addr;
 
 	shadow_row = (void *)round_down((unsigned long)shadow,
 					SHADOW_BYTES_PER_ROW)
@@ -237,6 +238,8 @@ static void print_shadow_for_address(const void *addr)
 		char buffer[4 + (BITS_PER_LONG/8)*2];
 		char shadow_buf[SHADOW_BYTES_PER_ROW];
 
+        if (is_content) kaddr = shadow_row;
+
 		snprintf(buffer, sizeof(buffer),
 			(i == 0) ? ">%px: " : " %px: ", kaddr);
 		/*
@@ -262,7 +265,7 @@ static bool report_enabled(void)
 {
 	if (current->kasan_depth)
 		return false;
-	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
+	if (true || test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
 		return true;
 	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
 }
@@ -270,6 +273,7 @@ static bool report_enabled(void)
 void kasan_report_invalid_free(void *object, unsigned long ip)
 {
 	unsigned long flags;
+    return;
 
 	start_report(&flags);
 	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", (void *)ip);
@@ -278,7 +282,7 @@ void kasan_report_invalid_free(void *object, unsigned long ip)
 	pr_err("\n");
 	print_address_description(object);
 	pr_err("\n");
-	print_shadow_for_address(object);
+	print_shadow_for_address(object, false);
 	end_report(&flags);
 }
 
@@ -288,6 +292,7 @@ void __kasan_report(unsigned long addr, size_t size, bool is_write, unsigned lon
 	void *tagged_addr;
 	void *untagged_addr;
 	unsigned long flags;
+    u8 shadow_byte;
 
 	if (likely(!report_enabled()))
 		return;
@@ -301,11 +306,15 @@ void __kasan_report(unsigned long addr, size_t size, bool is_write, unsigned lon
 	if (addr_has_shadow(untagged_addr))
 		info.first_bad_addr = find_first_bad_addr(tagged_addr, size);
 	else
-		info.first_bad_addr = untagged_addr;
+        return;
+		//info.first_bad_addr = untagged_addr;
 	info.access_size = size;
 	info.is_write = is_write;
 	info.ip = ip;
 
+    if (info.first_bad_addr == NULL) return;
+
+
 	start_report(&flags);
 
 	print_error_description(&info);
@@ -316,7 +325,9 @@ void __kasan_report(unsigned long addr, size_t size, bool is_write, unsigned lon
 	if (addr_has_shadow(untagged_addr)) {
 		print_address_description(untagged_addr);
 		pr_err("\n");
-		print_shadow_for_address(info.first_bad_addr);
+		print_shadow_for_address(info.first_bad_addr, true);
+		pr_err("\n");
+		print_shadow_for_address(info.first_bad_addr, false);
 	} else {
 		dump_stack();
 	}
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 01600d8..3cb33b8 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2169,10 +2169,12 @@ struct mempolicy *__mpol_dup(struct mempolicy *old)
 	/* task's mempolicy is protected by alloc_lock */
 	if (old == current->mempolicy) {
 		task_lock(current);
-		*new = *old;
+		memcpy(new, old, sizeof(struct mempolicy));
+		//*new = *old;
 		task_unlock(current);
 	} else
-		*new = *old;
+		memcpy(new, old, sizeof(struct mempolicy));
+		//*new = *old;
 
 	if (current_cpuset_is_being_rebound()) {
 		nodemask_t mems = cpuset_mems_allowed(current);
diff --git a/security/selinux/ss/mls.c b/security/selinux/ss/mls.c
index 5e05f5b..943489e 100644
--- a/security/selinux/ss/mls.c
+++ b/security/selinux/ss/mls.c
@@ -428,7 +428,8 @@ int mls_setup_user_range(struct policydb *p,
 		   only if the "fromcon" clearance dominates
 		   the user's computed sensitivity level) */
 		if (mls_level_dom(user_clr, fromcon_clr))
-			*usercon_clr = *fromcon_clr;
+			//*usercon_clr = *fromcon_clr;
+			memcpy(usercon_clr, fromcon_clr, sizeof(struct mls_level));
 		else if (mls_level_dom(fromcon_clr, user_clr))
 			*usercon_clr = *user_clr;
 		else
diff --git a/security/selinux/ss/sidtab.c b/security/selinux/ss/sidtab.c
index e63a90f..525f77b 100644
--- a/security/selinux/ss/sidtab.c
+++ b/security/selinux/ss/sidtab.c
@@ -40,8 +40,14 @@ int sidtab_init(struct sidtab *s)
 int sidtab_set_initial(struct sidtab *s, u32 sid, struct context *context)
 {
 	struct sidtab_isid_entry *entry;
+	//struct sidtab_isid_entry stack_entry;
 	int rc;
 
+    //printk("rc=%x", rc);
+    //printk("stack_entry.set=%x", stack_entry.set);
+    //printk("stack_entry.context.role=%x", stack_entry.context.role);
+    //dump_stack();
+
 	if (sid == 0 || sid > SECINITSID_NUM)
 		return -EINVAL;
 
